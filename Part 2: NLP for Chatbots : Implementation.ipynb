{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental Methods of NLP for Building Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "am VERB\n",
      "learning VERB\n",
      "how ADV\n",
      "to PART\n",
      "build VERB\n",
      "chatbots NOUN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en') #Loads the spacy en model into a python object\n",
    "doc = nlp(u'I am learning how to build chatbots') #Creates a doc object\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_) #prints the text and POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see, when we print the tokens from the returned Doc object from the method nlp, which is a container for accessing the annotations, we get the POS tagged with each of the words in the sentence.\n",
    "\n",
    "* These tags are the properties belonging to the word that determine the word is used in a grammatically correct sentence. We can use these tags as the word features in information filtering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+-----+--------+-------+-------+-------+\n",
      "|   Text   |  Lemma  | POS  | TAG |  DEP   | SHAPE | ALPHA |  Stop |\n",
      "+----------+---------+------+-----+--------+-------+-------+-------+\n",
      "|    I     |  -PRON- | PRON | PRP | nsubj  |   X   |  True | False |\n",
      "|    am    |    be   | VERB | VBP |  aux   |   xx  |  True |  True |\n",
      "| learning |  learn  | VERB | VBG |  ROOT  |  xxxx |  True | False |\n",
      "|   how    |   how   | ADV  | WRB | advmod |  xxx  |  True |  True |\n",
      "|    to    |    to   | PART |  TO |  aux   |   xx  |  True |  True |\n",
      "|  build   |  build  | VERB |  VB | xcomp  |  xxxx |  True | False |\n",
      "| chatbots | chatbot | NOUN | NNS |  dobj  |  xxxx |  True | False |\n",
      "+----------+---------+------+-----+--------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'I am learning how to build chatbots')\n",
    "t = PrettyTable(['Text', 'Lemma','POS','TAG','DEP','SHAPE','ALPHA','Stop'])\n",
    "for token in doc:\n",
    "    t.add_row([token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Refer to the below table to find out the meaning of each attribute we printed in the code.\n",
    "\n",
    "|TEXT | Actual text or word being processed |\n",
    "|-----|-------------------------------------|\n",
    "|LEMMA| Root form of the word being processed|\n",
    "|POS |Part-of-speech of the word|\n",
    "|TAG |They express the part-of-speech (e.g., VERB) and some amount ofmorphologicalinformation (e.g., that the verb is past tense).|\n",
    "|DEP |Syntactic dependency (i.e., the relation between tokens)|\n",
    "|SHAPE |Shape of the word (e.g., the capitalization, punctuation, digits format)|\n",
    "|ALPHA |Is the token an alpha character?|\n",
    "|Stop |Is the word a stop word or part of a stop list?|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chuckle']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "lemmatizer('chuckles', 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blaze']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('blazing', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('ran', 'VERB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n",
      "California GPE\n",
      "109.65 billion US dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "my_string = \"Google has its headquarters in Mountain View, California having revenue amounted to 109.65 billion US dollars\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark Zuckerberg PERSON\n",
      "May 14, 1984 DATE\n",
      "New York GPE\n",
      "American NORP\n",
      "Facebook ORG\n"
     ]
    }
   ],
   "source": [
    "my_string = u\"Mark Zuckerberg born May 14, 1984 in New York is an American technology entrepreneur and philanthropist best known for co-founding and leading Facebook as its chairman and CEO.\"\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'your', 'first', 'often', 'those', 'around', 'already', 'it', 'at', 'doing', 'more', 'over', 'full', 'whereupon', 'mine', 'latter', 'never', 'in', 'now', 'whom', 'between', 'can', 'but', 'well', 'quite', 'about', 'always', 'several', 'every', 'ten', 'along', 'only', 'somehow', 'whereafter', 'wherein', 'from', 'ourselves', 'hereupon', 'get', 'above', 'five', 'whither', 'whence', 'together', 'show', 'whoever', 'beside', 'thereafter', 'before', 'one', 'because', 'seeming', 'give', 'side', 'done', 'same', 'not', 'move', 'should', 'whereas', 'namely', 'nowhere', 'nothing', 'hers', 'everywhere', 'who', 'how', 'become', 'elsewhere', 'within', 'you', 'and', 'nobody', 'yourselves', 'afterwards', 'that', 'out', 'cannot', 'ca', 'perhaps', 'thus', 'anyone', 'behind', 'call', 'even', 'under', 'yet', 'someone', 'below', 'any', 'there', 'anyhow', 'keep', 'made', 'bottom', 'towards', 'once', 'much', 'unless', 'on', 'again', 'what', 'without', 'hundred', 'many', 'through', 'such', 'whatever', 'please', 'a', 'besides', 'has', 'him', 'noone', 'be', 'onto', 'than', 'i', 'she', 'since', 'becomes', 'top', 'during', 'third', 'my', 'front', 'hereafter', 'sometimes', 'mostly', 'regarding', 'otherwise', 'whole', 'although', 'twenty', 'among', 'might', 'toward', 'ever', 'another', 'we', 'least', 'others', 'each', 'seem', 'either', 'upon', 'thereby', 'though', 'into', 'myself', 'eight', 'fifteen', 'anyway', 'via', 'been', 'after', 'really', 'across', 'back', 'everything', 'for', 'further', 'the', 'this', 'whenever', 'most', 'thence', 'will', 'seemed', 'while', 'may', 'somewhere', 'per', 'an', 'is', 'have', 'however', 'so', 'hence', 'few', 'using', 'were', 'his', 'against', 'thereupon', 'anywhere', 'to', 'fifty', 'part', 'or', 'something', 'would', 'why', 'due', 'hereby', 'down', 'rather', 'two', 'herself', 'other', 'meanwhile', 'serious', 'some', 'these', 'they', 'which', 'are', 'themselves', 'go', 'herein', 'amount', 'else', 'beyond', 'nevertheless', 'none', 'himself', 'seems', 'whose', 'forty', 'thru', 'nine', 'therein', 'still', 'became', 'various', 'alone', 'whether', 'three', 'twelve', 'becoming', 'except', 'both', 'also', 'could', 'enough', 'neither', 'own', 'ours', 'indeed', 'us', 'almost', 'our', 'do', 'them', 'does', 'her', 'throughout', 'therefore', 'next', 'very', 'amongst', 'sixty', 'six', 'where', 'all', 'here', 'no', 'am', 'did', 'of', 'say', 'former', 'nor', 'too', 'by', 'when', 'itself', 'wherever', 'he', 'had', 'then', 'must', 'beforehand', 'as', 're', 'yourself', 'latterly', 'make', 'being', 'eleven', 'if', 'empty', 'last', 'their', 'was', 'me', 'put', 'up', 'whereby', 'four', 'everyone', 'name', 'just', 'its', 'off', 'take', 'less', 'until', 'anything', 'used', 'sometime', 'yours', 'moreover', 'formerly', 'see', 'with'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'with'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'alphaalpha'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Book me a flight from Bangalore to Goa')\n",
    "blr, goa = doc[5], doc[7]\n",
    "list(blr.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output can tell us that user is looking to book the flight from Bangalore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output can tell us that the user is looking to book the flight to Goa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Boston Dynamics, thousands, robot dogs]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"Boston Dynamics is gearing up to produce thousands of robot dogs\")\n",
    "list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning learning nsubj cracks\n",
      "the code code dobj cracks\n",
      "messenger RNAs RNAs pobj of\n",
      "potential potential dobj coding\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Deep learning cracks the code of messenger RNAs and protein-­ coding potential\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How [-0.2974264   0.7393958  -0.04001491  0.44034028  2.8967488 ]\n",
      "are [-0.23435098 -1.6145049   1.0197449   0.9928163   0.2822708 ]\n",
      "you [ 0.1025213 -3.5647118  2.4822786  4.2825     3.590246 ]\n",
      "doing [-0.6240919 -2.0210211 -0.9101493  2.7051926  4.189254 ]\n",
      "today [ 3.5409102 -0.6218593  2.6274266  2.050488   0.2019196]\n",
      "? [ 2.8914995  -0.25079104  3.3764172   1.6942688   1.9849054 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'How are you doing today?')\n",
    "for token in doc:\n",
    "    print(token.text, token.vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above not makes sence but shows imilarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7879069561264538\n",
      "0.41934263641148983\n"
     ]
    }
   ],
   "source": [
    "hello_doc = nlp(u\"hello\")\n",
    "hi_doc = nlp(u\"hi\")\n",
    "hella_doc = nlp(u\"hella\")\n",
    "print(hello_doc.similarity(hi_doc))\n",
    "print(hello_doc.similarity(hella_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the word hello, it is more related and similar to the word hi, even though\n",
    "there is only a difference of a character between the words hello and hella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word car is 100% similar to word car\n",
      "Word car is 71% similar to word truck\n",
      "Word car is 24% similar to word google\n",
      "Word truck is 71% similar to word car\n",
      "Word truck is 100% similar to word truck\n",
      "Word truck is 36% similar to word google\n",
      "Word google is 24% similar to word car\n",
      "Word google is 36% similar to word truck\n",
      "Word google is 100% similar to word google\n"
     ]
    }
   ],
   "source": [
    "example_doc = nlp(u\"car truck google\")\n",
    "for t1 in example_doc:\n",
    "    for t2 in example_doc:\n",
    "        similarity_perc = int(t1.similarity(t2) * 100)\n",
    "        print (\"Word {} is {}% similar to word {}\".format(t1.text,similarity_perc,  t2.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brexit\n",
      "is\n",
      "the\n",
      "impending\n",
      "withdrawal\n",
      "of\n",
      "the\n",
      "U.K.\n",
      "from\n",
      "the\n",
      "European\n",
      "Union\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Brexit is the impending withdrawal of the U.K. from the European Union.')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_from pattern matched correctly. Printing values\n",
      "\n",
      "From: AsiaWorld-Expo., To: Hong Kong Airport\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"Book me a metro from Airport Station to Hong Kong Station.\"\n",
    "sentence2 = \"Book me a cab to Hong Kong Airport from AsiaWorld-Expo.\"\n",
    "import re\n",
    "from_to = re.compile('.* from (.*) to (.*)')\n",
    "to_from = re.compile('.* to (.*) from (.*)')\n",
    "from_to_match = from_to.match(sentence2)\n",
    "to_from_match = to_from.match(sentence2)\n",
    "if from_to_match and from_to_match.groups():\n",
    "    _from = from_to_match.groups()[0]\n",
    "    _to = from_to_match.groups()[1]\n",
    "    print(\"from_to pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))\n",
    "elif to_from_match and to_from_match.groups():\n",
    "    _to = to_from_match.groups()[0]\n",
    "    _from = to_from_match.groups()[1]\n",
    "    print(\"to_from pattern matched correctly. Printing values\\n\")\n",
    "    print(\"From: {}, To: {}\".format(_from, _to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
